---
title: Blog Post 3 - Polling
author: R package build
date: '2022-09-27'
slug: []
categories: []
tags: []
---

Using Polling to Inform Election Forecasts

Last week, I built a model for the 2022 midterm elections based on the historical relationship between change in Real Disposable Income and election outcome (either Popular Vote Share or Seat Share). While that model was far from perfect and exhibited a great degree of uncertainty, by limiting the data to midterm years only, I was able to raise the r-squared value to 0.47 indicating that change in RDI from Q8-Q7 in an election year has considerable predictive power. But of course, that model left much to be desired suggesting that other predictive variables should be considered to garner more accurate predictions. One of the potential added variables is polling data. 

Incorporating Polls - 538 vs. The Economist

Similar to the wide array of variables and subsets considered last week when narrowing down an economic variable to model based off of, there is no single way to incorporate polling data into our prediction models. For example, both 538 and The Economist base their models on polling data but the exact methods for how they incorporate polls vary. 

538 produces three different models for each prediction — Lite, Classic, and Deluxe. However, each of these models uses polls in slightly different ways. The Lite model is a “polls-only” assessment of the election.  In practice, the data is of course a bit more complicated than that with the addition of the CANTOR system to fill in polling gaps, but in essence lite models the relationship between polling and outcome.  The Classic version again uses polling but also adds in terms for “the fundamentals” many of which we have already discussed in this blog, like Incumbency and the economy. Silvers doesn’t go into detail about each and every term but notes that “incumbency, past voting history in the state or district, fundraising and the generic ballot” are the most important factors and as such they are weighted to a greater degree than other variables (Silvers 2022). Finally, the deluxe model adds in ratings from elections experts who take a more subjective and qualitative look at predicticting individual races.

The Economist Similarly uses a combination of Polling data and Fundamentals to generate their predictions. The Economist cites the generic-ballot as “the single best indicator” of election outcomes in Congressional races. This is reflected in their model which really centers on the generic ballot and builds out from there to incorporate different variables considered in “the fundamentals.” With the generic ballot base, The Economist then uses individual polls to refine and “nudge” their model to be more accurate (Morris 2020). The Economists model also seems to weight polls conducted closer to the election day more heavily.  

I generally find The Economist’s methodology to be more compelling than 538’s.  The Economist seemingly places less value on individual polls and extrapolating (like 538’s CANTOR system).  I’m somewhat skeptical about the assertion that what's true in one district's individual polls can easily be transferred to similar districts.  This is both because the districts where data is readily available are inherently confounded by the fact that they likely either house large universities (who conduct polls) or are particularly competitive. 


```{r, include = FALSE, message = FALSE, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(readr)
library(stargazer)
```

```{r, , eval = TRUE, echo = FALSE, warning = FALSE, message= FALSE, Include = FALSE}
poll_df <- read_csv("polls_df.csv")
popvote_df <- read_csv('house_popvote_seats.csv')
popvote <- popvote_df %>%
  select('year', 'winner_party', 'H_incumbent_party', 'H_incumbent_party_majorvote_pct','D_majorvote_pct', 'R_majorvote_pct')

RDI <- read_csv('RDI_quarterly.csv')
  
RDI <- RDI %>%
  select('year', 'quarter_cycle', 'DSPIC_change_pct') %>% 
  filter(year == 1982 | year == 1986 | year == 1982 | year == 1986 | year == 1990 | year == 1994 | year == 1994 | year == 1998 | year == 2002 | year == 2006 | year == 2010 | year == 2014 | year == 2018 | year == 2022) %>%  
  filter(quarter_cycle == 8) 
  
pollsRDI <- popvote_df %>% 
    full_join(poll_df %>% 
                  filter(bmonth >= 9) %>% 
                  group_by(year, party) %>% 
                  summarise(avg_support=mean(support))) %>% 
    left_join(RDI)
pollsRDI <- pollsRDI %>%
  drop_na()

```


```{r,eval = TRUE, echo = FALSE, warning = FALSE, message= FALSE, Include = FALSE}
##fundamentals model
dat_D <- pollsRDI[pollsRDI$party == 'D',]
dat_R <- pollsRDI[pollsRDI$party == 'R',]
mod_RDI_D <- lm(D_majorvote_pct ~ DSPIC_change_pct, data = dat_D)
mod_RDI_R <- lm(R_majorvote_pct ~ DSPIC_change_pct, data = dat_R)
stargazer(mod_RDI_D, type = 'text')
stargazer(mod_RDI_R, type = 'text')
```


```{r, eval = TRUE, echo = FALSE, warning = FALSE, message= FALSE, Include = FALSE}
##adjusted polls-only model
mod_polling_D <- lm(D_majorvote_pct ~ avg_support, data = dat_D)
mod_polling_R <- lm(R_majorvote_pct ~ avg_support, data = dat_R)
stargazer(mod_polling_D, type = 'text')
stargazer(mod_polling_R, type = 'text')
```

```{r, eval = TRUE, echo = FALSE, warning = FALSE, message= FALSE, Include = FALSE}
## adjusted polls + fundamentals model
mod_pollRDI_D <- lm(D_majorvote_pct ~ avg_support + DSPIC_change_pct, data = dat_D)
mod_pollRDI_R <- lm(R_majorvote_pct ~ avg_support + DSPIC_change_pct, data = dat_R)
stargazer(mod_pollRDI_D, type = 'text')
stargazer(mod_pollRDI_R, type = 'text')
```


```{r,eval = TRUE, echo = FALSE, warning = FALSE, message= FALSE, Include = FALSE}
D2022 <- data.frame(DSPIC_change_pct = c(-2.00319494), avg_support = 45.1)
R2022 <- data.frame(DSPIC_change_pct = c(-2.00319494), avg_support = 43.8)

predict(mod_pollRDI_D, newdata = D2022)
predict(mod_pollRDI_R, newdata = R2022)
```


